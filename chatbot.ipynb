{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exdsgift/SocialResearch/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KwQyzgRslkM",
        "outputId": "e9afd259-b846-43f6-8bea-7d97a6304351"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SnQ1KWos9Db",
        "outputId": "4c0aaa97-be8c-4e73-a319-33e04b25ab27"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Github Repository\n",
        "!git clone https://github.com/exdsgift/SocialResearch\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmwEFCycMYgk",
        "outputId": "c6f66a71-164d-4f39-9117-ffcc50862ef8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Verifica se la GPU è disponibile\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Usa la GPU\n",
        "    print(\"GPU disponibile:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Usa la CPU\n",
        "    print(\"GPU non disponibile, verrà utilizzata la CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsgpz0_ANFqL",
        "outputId": "0f3c47a7-84f0-4343-ae26-a9d1969a3c00"
      },
      "outputs": [],
      "source": [
        "pip install -q jupyter_bokeh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "P2e-YWOKE2v-",
        "outputId": "9e5438a2-6e33-43bd-bb8b-afa6183fe650"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import panel as pn\n",
        "import param\n",
        "\n",
        "# Initialize Panel extension\n",
        "pn.extension()\n",
        "\n",
        "# Load Hugging Face model and tokenizer\n",
        "def load_llm(model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Ensure pad_token is set\n",
        "\n",
        "    hf_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.1,\n",
        "        truncation=True,  # Ensure truncation is enabled\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Load and process documents\n",
        "def load_db(file, chain_type, k, llm):\n",
        "    # Load and split documents\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "\n",
        "    # Create conversational retrieval chain\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=chain_type,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        return_generated_question=True,\n",
        "    )\n",
        "    return qa\n",
        "\n",
        "# Chatbot class\n",
        "class ChatBot(param.Parameterized):\n",
        "    chat_history = param.List([])\n",
        "    answer = param.String(\"\")\n",
        "    db_query = param.String(\"\")\n",
        "    db_response = param.List([])\n",
        "\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(**params)\n",
        "        self.panels = []\n",
        "        self.loaded_file = \"references/daminov-2024-relationship-between-the-type-of-media-consumption-and-political-trust-in-the-european-union-evidence-from.pdf\"\n",
        "        self.llm = load_llm()\n",
        "        self.qa = load_db(self.loaded_file, \"stuff\", 4, self.llm)\n",
        "\n",
        "    def call_load_db(self, count):\n",
        "        if count == 0 or not file_input.value:\n",
        "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "        else:\n",
        "            file_input.save(\"temp.pdf\")\n",
        "            self.loaded_file = file_input.filename\n",
        "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4, self.llm)\n",
        "            button_load.button_style = \"solid\"\n",
        "        self.clr_history()\n",
        "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "\n",
        "    def is_repetitive(self, text, threshold=3):\n",
        "        words = text.split()\n",
        "        for i in range(len(words) - threshold):\n",
        "            if words[i:i+threshold] == words[i+threshold:i+2*threshold]:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def convchain(self, query):\n",
        "        if not query:\n",
        "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
        "\n",
        "        try:\n",
        "            result = self.qa.invoke({\"question\": query, \"chat_history\": self.chat_history})\n",
        "            answer = result[\"answer\"]\n",
        "\n",
        "            # Check for repetitive output\n",
        "            if self.is_repetitive(answer):  # Correctly call the method\n",
        "                answer = \"I'm sorry, I seem to be repeating myself. Could you please rephrase your question?\"\n",
        "\n",
        "            self.chat_history.extend([(query, answer)])\n",
        "            self.db_query = result[\"generated_question\"]\n",
        "            self.db_response = result[\"source_documents\"]\n",
        "            self.answer = answer\n",
        "            self.panels.extend([\n",
        "                pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
        "                pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600))\n",
        "            ])\n",
        "            inp.value = ''  # Clear input field\n",
        "            return pn.WidgetBox(*self.panels, scroll=True)\n",
        "        except Exception as e:\n",
        "            self.answer = f\"Error: {str(e)}\"\n",
        "            return pn.WidgetBox(pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, styles={'background-color': '#FFCCCB'})))\n",
        "\n",
        "    @param.depends('db_query')\n",
        "    def get_lquest(self):\n",
        "        if not self.db_query:\n",
        "            return pn.Column(\n",
        "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
        "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
        "            )\n",
        "        return pn.Column(\n",
        "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
        "            pn.pane.Str(self.db_query)\n",
        "        )\n",
        "\n",
        "    @param.depends('db_response')\n",
        "    def get_sources(self):\n",
        "        if not self.db_response:\n",
        "            return\n",
        "        rlist = [pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for doc in self.db_response:\n",
        "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    @param.depends('chat_history')\n",
        "    def get_chats(self):\n",
        "        if not self.chat_history:\n",
        "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
        "        rlist = [pn.Row(pn.pane.Markdown(f\"Current Chat History variable\"))]\n",
        "        for exchange in self.chat_history:\n",
        "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    def clr_history(self, count=0):\n",
        "        self.chat_history = []\n",
        "        return\n",
        "\n",
        "# Instantiate the chatbot\n",
        "cb = ChatBot()\n",
        "\n",
        "# Create Panel widgets\n",
        "file_input = pn.widgets.FileInput(accept='.pdf')\n",
        "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
        "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
        "inp = pn.widgets.TextInput(placeholder='Enter text here…')\n",
        "\n",
        "# Bind widgets to functions\n",
        "button_clearhistory.on_click(cb.clr_history)\n",
        "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
        "conversation = pn.bind(cb.convchain, inp)\n",
        "\n",
        "# Create dashboard tabs\n",
        "tab1 = pn.Column(\n",
        "    pn.Row(inp),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(conversation, loading_indicator=True, height=300),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "\n",
        "tab2 = pn.Column(\n",
        "    pn.panel(cb.get_lquest),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(cb.get_sources),\n",
        ")\n",
        "\n",
        "tab3 = pn.Column(\n",
        "    pn.panel(cb.get_chats),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "\n",
        "tab4 = pn.Column(\n",
        "    pn.Row(file_input, button_load, bound_button_load),\n",
        "    pn.Row(button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\")),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "\n",
        "# Assemble dashboard\n",
        "dashboard = pn.Column(\n",
        "    pn.Row(pn.pane.Markdown('# SocialResearch Bot! Ask me something!')),\n",
        "    pn.Tabs(\n",
        "        ('Conversation', tab1),\n",
        "        ('Database', tab2),\n",
        "        ('Chat History', tab3),\n",
        "        ('Configure', tab4)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Serve the dashboard\n",
        "dashboard.servable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asj-gsRSE2wB"
      },
      "source": [
        "### Simple version (not really working)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0eVFgy8Kpfc",
        "outputId": "b79f3766-63af-4180-ee56-4ef0f7ee0408"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain chromadb transformers sentence-transformers PyPDF2 langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNg0kDzOHbzS",
        "outputId": "a26d6f2c-f76f-4d02-fbce-dd56e47640d6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Verifica se la GPU è disponibile\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Usa la GPU\n",
        "    print(\"GPU disponibile:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Usa la CPU\n",
        "    print(\"GPU non disponibile, verrà utilizzata la CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr5M9xllE2wC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Configurazione del database vettoriale\n",
        "persist_directory = 'docs/chroma/'\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "\n",
        "# 2. Funzione per estrarre testo da un file PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# 3. Cartella contenente i file PDF\n",
        "pdf_folder = '/content/drive/MyDrive/Github Repository/SocialResearch/references'  # Sostituisci con il percorso della tua cartella\n",
        "\n",
        "# 4. Leggi tutti i file PDF nella cartella e estrai il testo\n",
        "documents = []\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "        print(f\"Processing {filename}...\")\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        documents.append(text)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# 5. Aggiungi i documenti al database vettoriale\n",
        "vectordb.add_texts(documents)  # Usa add_texts per liste di stringhe\n",
        "\n",
        "# 6. Salva il database (opzionale)\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M2JyrUzE2wC",
        "outputId": "c38b9a76-989a-49c0-c80b-52a1d246f478"
      },
      "outputs": [],
      "source": [
        "question = \"Trust is important in politics?\"\n",
        "docs = vectordb.similarity_search(question,k=4)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5rPKZVcE2wE",
        "outputId": "bee459a6-0550-4303-d42a-f4e3718226a8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Define a prompt\n",
        "prompt = \"Who is Elon Musk\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=200,  # Limit the number of new tokens generated\n",
        "    #top_p=0.95,          # Use nucleus sampling (top-p sampling)\n",
        "    temperature=0.1      # Control the randomness of predictions\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RufnvFUZE2wF"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "question = \"Trust is important in politics?\"\n",
        "qa_chain = RetrievalQA.from_chain_type(model,\n",
        "                                       retriever=vectordb.as_retriever(),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
        "\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oAurOMXCDcG"
      },
      "source": [
        "### Compact Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVCODezpDXyn",
        "outputId": "d61d2230-e8b7-448d-a839-5e2caf08a6e0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# 1. Configurazione del database vettoriale\n",
        "persist_directory = 'docs/chroma/'\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "\n",
        "# 2. Funzione per estrarre testo da un file PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# 3. Cartella contenente i file PDF\n",
        "pdf_folder = '/content/drive/MyDrive/Github Repository/SocialResearch/references'  # Sostituisci con il percorso della tua cartella\n",
        "\n",
        "# 4. Leggi tutti i file PDF nella cartella e estrai il testo\n",
        "documents = []\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "        print(f\"Processing {filename}...\")\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        documents.append(Document(page_content=text))  # Convert text to Document object\n",
        "\n",
        "# 5. Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# 6. Aggiungi i documenti al database vettoriale\n",
        "vectordb.add_texts([doc.page_content for doc in split_documents])  # Extract text from Document objects\n",
        "\n",
        "# 7. Salva il database (opzionale)\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J0BxWq-JEqy",
        "outputId": "c3e3849e-905a-4d4a-bd2e-5fa018e9f0fe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Check GPU availability and move model to device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Move the model to the specified device\n",
        "model.to(device)\n",
        "\n",
        "# Define a prompt template\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. Write only the answer, without any oders informations! Dont report the prompt for any reasons!\n",
        "{context}.\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "# Wrap the model in a HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if device == \"cuda\" else -1,\n",
        "    max_new_tokens=100,\n",
        "    top_p=0.95,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Run chain\n",
        "question = \"Trust is important in politics?\"\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=False,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "# Get the result\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "# Print only the question and the response\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Response: {result['result']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
